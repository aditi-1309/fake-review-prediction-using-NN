{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "s5uEs3kYwKQo"
      },
      "outputs": [],
      "source": [
        "# One time installation of necessary libraries\n",
        "#!pip install spacy\n",
        "#!pip install tensorflow, keras\n",
        "#!pip install nltk\n",
        "#!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMnpNrw3ws3Q",
        "outputId": "caae7d91-48ba-4efa-be54-0c7268a94932"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9agjmwOwKQr",
        "outputId": "d5d6a035-f7aa-4d2b-9abd-c23a3483bee8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Importing all necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.preprocessing import sequence\n",
        "from keras.layers import TimeDistributed, GlobalAveragePooling1D, GlobalAveragePooling2D, BatchNormalization\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers.convolutional import Conv1D, MaxPooling1D, Conv2D, MaxPooling2D, AveragePooling1D\n",
        "from keras.layers import Dropout, Flatten, Bidirectional, Dense, Activation, TimeDistributed\n",
        "from keras.models import Model, Sequential\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from string import ascii_lowercase\n",
        "from collections import Counter\n",
        "from gensim.models import Doc2Vec\n",
        "from gensim.models import doc2vec\n",
        "from gensim.models import KeyedVectors\n",
        "import itertools, nltk, snowballstemmer, re\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "from gensim.models import KeyedVectors\n",
        "TaggedDocument = doc2vec.TaggedDocument\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MbLofNmEwKQs"
      },
      "outputs": [],
      "source": [
        "# Reading data into a dataframe\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Practicum/Amazon Reviews Dataset.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yknMdeTSwKQs"
      },
      "outputs": [],
      "source": [
        "# Converting polarity and deceptive field to binary\n",
        "df['polarity'] = np.where(df['polarity']=='positive', 1, 0)\n",
        "df['deceptive'] = np.where(df['deceptive']=='truthful', 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BbQIPg56wKQt"
      },
      "outputs": [],
      "source": [
        "# Classes and functions used for text vectorization\n",
        "class LabeledLineSentence(object):\n",
        "    def __init__(self, sources):\n",
        "        self.sources = sources\n",
        "        \n",
        "        flipped = {}\n",
        "        \n",
        "        # Checking uniqueness of the key\n",
        "        for key, value in sources.items():\n",
        "            if value not in flipped:\n",
        "                flipped[value] = [key]\n",
        "            else:\n",
        "                raise Exception('Non-unique prefix encountered')\n",
        "    \n",
        "    def __iter__(self):\n",
        "        for source, prefix in self.sources.items():\n",
        "            with utils.smart_open(source) as fin:\n",
        "                for item_no, line in enumerate(fin):\n",
        "                    yield TaggedDocument(utils.to_unicode(line).split(), [prefix + '_%s' % item_no])\n",
        "    \n",
        "    def to_array(self):\n",
        "        self.sentences = []\n",
        "        for source, prefix in self.sources.items():\n",
        "            with utils.smart_open(source) as fin:\n",
        "                for item_no, line in enumerate(fin):\n",
        "                    self.sentences.append(TaggedDocument(utils.to_unicode(line).split(), [prefix + '_%s' % item_no]))\n",
        "        return self.sentences\n",
        "    \n",
        "    def sentences_perm(self):\n",
        "        shuffled = list(self.sentences)\n",
        "        random.shuffle(shuffled)\n",
        "        return shuffled\n",
        "\n",
        "def create_class(c):\n",
        "    if c['polarity'] == 1 and c['deceptive'] == 1:\n",
        "        return [1,1]\n",
        "    elif c['polarity'] == 1 and c['deceptive'] == 0:\n",
        "        return [1,0]\n",
        "    elif c['polarity'] == 0 and c['deceptive'] == 1:\n",
        "        return [0,1]\n",
        "    else:\n",
        "        return [0,0]\n",
        "    \n",
        "def specific_class(c):\n",
        "    if c['polarity'] == 1 and c['deceptive'] == 1:\n",
        "        return \"TRUE_POSITIVE\"\n",
        "    elif c['polarity'] == 1 and c['deceptive'] == 0:\n",
        "        return \"FALSE_POSITIVE\"\n",
        "    elif c['polarity'] == 0 and c['deceptive'] == 1:\n",
        "        return \"TRUE_NEGATIVE\"\n",
        "    else:\n",
        "        return \"FALSE_NEGATIVE\"\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "df['final_class'] = df.apply(create_class, axis=1)\n",
        "df['given_class'] = df.apply(specific_class, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bZiJhiiUwKQu"
      },
      "outputs": [],
      "source": [
        "Y =df['final_class'].astype(\"string\")\n",
        "\n",
        "# Encoding class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "encoded_Y = encoder.transform(Y)\n",
        "\n",
        "# Converting integers to dummy variables\n",
        "dummy_y = np_utils.to_categorical(encoded_Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WWpuQpEqwKQu"
      },
      "outputs": [],
      "source": [
        "# Treating each review as a document\n",
        "textData = pd.DataFrame(list(df['text']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ufSDy0K7wKQv"
      },
      "outputs": [],
      "source": [
        "# Initializing stemmer\n",
        "stemmer = snowballstemmer.EnglishStemmer()\n",
        "\n",
        "# Getting stopword list\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "# Extending stopword list\n",
        "stop.extend(['may','also','zero','one','two','three','four','five','six','seven','eight','nine','ten','across','among','beside','however','yet','within']+list(ascii_lowercase))\n",
        "stoplist = stemmer.stemWords(stop)\n",
        "\n",
        "# Converting it into a set for later\n",
        "stoplist = set(stoplist)\n",
        "stop = set(sorted(stop + list(stoplist))) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "M4WTc4QHwKQv"
      },
      "outputs": [],
      "source": [
        "# Removing characters and stoplist words\n",
        "textData[0].replace('[!\"#%\\'()*+,-./:;<=>?@\\[\\]^_`{|}~1234567890’”“′‘\\\\\\]',' ',inplace=True,regex=True)\n",
        "\n",
        "# Generating dictionary of unique words\n",
        "wordlist = filter(None, \" \".join(list(set(list(itertools.chain(*textData[0].str.split(' ')))))).split(\" \"))\n",
        "df['stemmed_text_data'] = [' '.join(filter(None,filter(lambda word: word not in stop, line))) for line in textData[0].str.lower().str.split(' ')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0w8lpHY6wKQw"
      },
      "outputs": [],
      "source": [
        "# Removing all words that are appearing less frequently\n",
        "minimum_count = 1\n",
        "str_frequencies = pd.DataFrame(list(Counter(filter(None,list(itertools.chain(*df['stemmed_text_data'].str.split(' '))))).items()),columns=['word','count'])\n",
        "low_frequency_words = set(str_frequencies[str_frequencies['count'] < minimum_count]['word'])\n",
        "df['stemmed_text_data'] = [' '.join(filter(None,filter(lambda word: word not in low_frequency_words, line))) for line in df['stemmed_text_data'].str.split(' ')]\n",
        "df['stemmed_text_data'] = [\" \".join(stemmer.stemWords(re.sub('[!\"#%\\'()*+,-./:;<=>?@\\[\\]^_`{|}~1234567890’”“′‘\\\\\\]',' ', next_text).split(' '))) for next_text in df['stemmed_text_data']]    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XKDdVeZRwKQw"
      },
      "outputs": [],
      "source": [
        "# Lemmatizing the text data\n",
        "lmtzr = WordNetLemmatizer()\n",
        "w = re.compile(\"\\w+\",re.I)\n",
        "\n",
        "def label_sentences(df, input_point):\n",
        "    labeled_sentences = []\n",
        "    list_sen = []\n",
        "    for index, datapoint in df.iterrows():\n",
        "        tokenized_words = re.findall(w,datapoint[input_point].lower())\n",
        "        labeled_sentences.append(TaggedDocument(words=tokenized_words, tags=['SENT_%s' %index]))\n",
        "        list_sen.append(tokenized_words)\n",
        "    return labeled_sentences, list_sen\n",
        "\n",
        "def train_doc2vec_model(labeled_sentences):\n",
        "    model = Doc2Vec(min_count=1, window=9, vector_size=512, sample=1e-4, negative=5, workers=7)\n",
        "    model.build_vocab(labeled_sentences)\n",
        "    pretrained_weights = model.wv.vectors\n",
        "    vocab_size, embedding_size = pretrained_weights.shape\n",
        "    model.train(labeled_sentences, total_examples=vocab_size, epochs=400)\n",
        "    \n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-CtoRVF-wKQx"
      },
      "outputs": [],
      "source": [
        "textData = df['stemmed_text_data'].to_frame().reset_index()\n",
        "sen, corpus = label_sentences(textData, 'stemmed_text_data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "fUjFbSQjwKQx"
      },
      "outputs": [],
      "source": [
        "# Training the Doc2Vec model\n",
        "doc2vec_model = train_doc2vec_model(sen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "gueElu1IwKQx"
      },
      "outputs": [],
      "source": [
        "# Saving the Doc2Vec model\n",
        "doc2vec_model.save(\"doc2vec_model_opinion_corpus.d2v\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "9ptP3I3-wKQy"
      },
      "outputs": [],
      "source": [
        "# Loading the Doc2Vec model\n",
        "doc2vec_model = Doc2Vec.load(\"doc2vec_model_opinion_corpus.d2v\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "E2h5-7oWwKQy"
      },
      "outputs": [],
      "source": [
        "# Further vectorizing the data\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "tfidf1 = TfidfVectorizer(tokenizer=lambda i:i, lowercase=False, ngram_range=(1,1))\n",
        "result_train1 = tfidf1.fit_transform(corpus)\n",
        "\n",
        "tfidf2 = TfidfVectorizer(tokenizer=lambda i:i, lowercase=False, ngram_range=(1,2))\n",
        "result_train2 = tfidf2.fit_transform(corpus)\n",
        "\n",
        "tfidf3 = TfidfVectorizer(tokenizer=lambda i:i, lowercase=False, ngram_range=(1,3))\n",
        "result_train3 = tfidf3.fit_transform(corpus)\n",
        "\n",
        "svd = TruncatedSVD(n_components=512, n_iter=40, random_state=34)\n",
        "tfidf_data1 = svd.fit_transform(result_train1)\n",
        "tfidf_data2 = svd.fit_transform(result_train2)\n",
        "tfidf_data3 = svd.fit_transform(result_train3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "JAn3n4nkwKQy"
      },
      "outputs": [],
      "source": [
        "# Performing POS tagging on the data\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.blank('en')\n",
        "temp_textData = pd.DataFrame(list(df['text']))\n",
        "\n",
        "overall_pos_tags_tokens = []\n",
        "overall_pos = []\n",
        "overall_tokens = []\n",
        "overall_dep = []\n",
        "\n",
        "for i in range(1600):\n",
        "    doc = nlp(temp_textData[0][i])\n",
        "    given_pos_tags_tokens = []\n",
        "    given_pos = []\n",
        "    given_tokens = []\n",
        "    given_dep = []\n",
        "    for token in doc:\n",
        "        output = \"%s_%s\" % (token.pos_, token.tag_)\n",
        "        given_pos_tags_tokens.append(output)\n",
        "        given_pos.append(token.pos_)\n",
        "        given_tokens.append(token.tag_)\n",
        "        given_dep.append(token.dep_)\n",
        "        \n",
        "    overall_pos_tags_tokens.append(given_pos_tags_tokens)\n",
        "    overall_pos.append(given_pos)\n",
        "    overall_tokens.append(given_tokens)\n",
        "    overall_dep.append(given_dep)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSN9ccwMwKQz",
        "outputId": "6bcf4f92-bc13-488d-981f-952f3a2ce52c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "# Normalizing the data\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "count = CountVectorizer(tokenizer=lambda i:i, lowercase=False)\n",
        "pos_tags_data = count.fit_transform(overall_pos_tags_tokens).todense()\n",
        "pos_data = count.fit_transform(overall_pos).todense()\n",
        "tokens_data = count.fit_transform(overall_tokens).todense()\n",
        "dep_data = count.fit_transform(overall_dep).todense()\n",
        "min_max_scaler = MinMaxScaler()\n",
        "normalized_pos_tags_data = min_max_scaler.fit_transform(pos_tags_data)\n",
        "normalized_pos_data = min_max_scaler.fit_transform(pos_data)\n",
        "normalized_tokens_data = min_max_scaler.fit_transform(tokens_data)\n",
        "normalized_dep_data = min_max_scaler.fit_transform(dep_data)\n",
        "\n",
        "final_pos_tags_data = np.zeros(shape=(1600, 512)).astype(np.float32)\n",
        "final_pos_data = np.zeros(shape=(1600, 512)).astype(np.float32)\n",
        "final_tokens_data = np.zeros(shape=(1600, 512)).astype(np.float32)\n",
        "final_dep_data = np.zeros(shape=(1600, 512)).astype(np.float32)\n",
        "final_pos_tags_data[:normalized_pos_tags_data.shape[0],:normalized_pos_tags_data.shape[1]] = normalized_pos_tags_data\n",
        "final_pos_data[:normalized_pos_data.shape[0],:normalized_pos_data.shape[1]] = normalized_pos_data\n",
        "final_tokens_data[:normalized_tokens_data.shape[0],:normalized_tokens_data.shape[1]] = normalized_tokens_data\n",
        "final_dep_data[:normalized_dep_data.shape[0],:normalized_dep_data.shape[1]] = normalized_dep_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4Zcv1NUwKQz",
        "outputId": "1bb92a09-4fa6-45a5-b371-6e3ea0e1b308"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "370\n"
          ]
        }
      ],
      "source": [
        "# Finding max no of words for each review\n",
        "\n",
        "maxlength = []\n",
        "for i in range(0,len(sen)):\n",
        "    maxlength.append(len(sen[i][0]))\n",
        "    \n",
        "print(max(maxlength))   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9A4K5qqJwKQz",
        "outputId": "0e201172-07b5-4aee-e6a4-1593f2f84cc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   index                                  stemmed_text_data  \\\n",
            "0      0  stay night getaway famili thursday tripl aaa r...   \n",
            "1      1  tripl rate upgrad view room less $ includ brea...   \n",
            "\n",
            "                                 vectorized_comments  \n",
            "0  [0.73591495, 0.43036973, -0.45590287, 0.104917...  \n",
            "1  [1.030476, -0.25929967, 0.3994606, 0.264935, -...  \n"
          ]
        }
      ],
      "source": [
        "# Function to vectorize\n",
        "def vectorize_comments(df,d2v_model):\n",
        "    y = []\n",
        "    comments = []\n",
        "    for i in range(0,df.shape[0]):\n",
        "        label = 'SENT_%s' %i\n",
        "        comments.append(d2v_model.docvecs[label])\n",
        "    df['vectorized_comments'] = comments\n",
        "    return df\n",
        "\n",
        "\n",
        "textData = vectorize_comments(textData,doc2vec_model)\n",
        "print (textData.head(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "J1MkhaY8wKQ0"
      },
      "outputs": [],
      "source": [
        "# Splitting the reviews into train-test split randomly\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "X_train, X_test, y_train, y_test = train_test_split(textData[\"vectorized_comments\"].T.tolist(), \n",
        "                                                                     dummy_y, \n",
        "                                                                    test_size=0.1, \n",
        "                                                                     random_state=56)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "VriQpon7wKQ0"
      },
      "outputs": [],
      "source": [
        "# Reshaping the vectorized data to fit the input of the model\n",
        "\n",
        "X = np.array(textData[\"vectorized_comments\"].T.tolist()).reshape((1,1600,512))\n",
        "y = np.array(dummy_y).reshape((1600,4))\n",
        "X_train2 = np.array(X_train).reshape((1,1440,512))\n",
        "y_train2 = np.array(y_train).reshape((1,1440,4))\n",
        "X_test2 = np.array(X_test).reshape((1,160,512))\n",
        "y_test2 = np.array(y_test).reshape((1,160,4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "3iPJByRPwKQ0"
      },
      "outputs": [],
      "source": [
        "# Stratified K fold repeated 10 times for cross validation\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "Xtemp = textData[\"vectorized_comments\"].T.tolist()\n",
        "ytemp = df['given_class']\n",
        "training_indices = []\n",
        "testing_indices = []\n",
        "\n",
        "skf = StratifiedKFold(n_splits=10)\n",
        "skf.get_n_splits(Xtemp, ytemp)\n",
        "\n",
        "for train_index, test_index in skf.split(Xtemp, ytemp):\n",
        "    training_indices.append(train_index)\n",
        "    testing_indices.append(test_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "jNWMwX6ZwKQ0"
      },
      "outputs": [],
      "source": [
        "def extractTrainingAndTestingData(givenIndex):\n",
        "    X_train3 = np.zeros(shape=(1440, max(maxlength)+10+379, 512)).astype(np.float32)\n",
        "    Y_train3 = np.zeros(shape=(1440, 4)).astype(np.float32)\n",
        "    X_test3 = np.zeros(shape=(160, max(maxlength)+10+379, 512)).astype(np.float32)\n",
        "    Y_test3 = np.zeros(shape=(160, 4)).astype(np.float32)\n",
        "\n",
        "    empty_word = np.zeros(512).astype(np.float32)\n",
        "\n",
        "    cnt_i = 0\n",
        "    for i in training_indices[givenIndex]:\n",
        "        len1 = len(sen[i][0])\n",
        "        avg_v1 = np.zeros(512).astype(np.float32)\n",
        "        avg_v2 = np.zeros(512).astype(np.float32)\n",
        "        avg_v3 = np.zeros(512).astype(np.float32)\n",
        "        for j in range(max(maxlength)+10+379):\n",
        "            if j < len1:\n",
        "                X_train3[cnt_i,j,:] = doc2vec_model[sen[i][0][j]]\n",
        "                avg_v1 += result_train1[i, tfidf1.vocabulary_[sen[i][0][j]]] * doc2vec_model[sen[i][0][j]]\n",
        "                avg_v2 += result_train2[i, tfidf2.vocabulary_[sen[i][0][j]]] * doc2vec_model[sen[i][0][j]]\n",
        "                avg_v3 += result_train3[i, tfidf3.vocabulary_[sen[i][0][j]]] * doc2vec_model[sen[i][0][j]]\n",
        "            #elif j >= len1 and j < len1 + 379:\n",
        "                #X_train3[cnt_i,j,:] = glove_data[i, j-len1, :]\n",
        "            elif j == len1 + 379:\n",
        "                X_train3[cnt_i,j,:] = tfidf_data1[i]\n",
        "            elif j == len1 + 380:\n",
        "                X_train3[cnt_i,j,:] = tfidf_data2[i]\n",
        "            elif j == len1+381:\n",
        "                X_train3[cnt_i,j,:] = tfidf_data3[i]\n",
        "            elif j == len1+382:\n",
        "                X_train3[cnt_i,j,:] = avg_v1\n",
        "            elif j == len1+383:\n",
        "                X_train3[cnt_i,j,:] = avg_v2\n",
        "            elif j == len1+384:\n",
        "                X_train3[cnt_i,j,:] = avg_v3\n",
        "            elif j == len1+385:\n",
        "                X_train3[cnt_i,j,:] = final_pos_tags_data[i] \n",
        "            elif j == len1+386:\n",
        "                X_train3[cnt_i,j,:] = final_pos_data[i]\n",
        "            elif j == len1+387:\n",
        "                X_train3[cnt_i,j,:] = final_tokens_data[i]\n",
        "            elif j == len1+388:\n",
        "                X_train3[cnt_i,j,:] = final_dep_data[i]\n",
        "            else:\n",
        "                X_train3[cnt_i,j,:] = empty_word\n",
        "\n",
        "        Y_train3[cnt_i,:] = dummy_y[i]\n",
        "        cnt_i += 1\n",
        "\n",
        "\n",
        "    cnt_i = 0\n",
        "    for i in testing_indices[givenIndex]:\n",
        "        len1 = len(sen[i][0])\n",
        "        avg_v1 = np.zeros(512).astype(np.float32)\n",
        "        avg_v2 = np.zeros(512).astype(np.float32)\n",
        "        avg_v3 = np.zeros(512).astype(np.float32)\n",
        "        for j in range(max(maxlength)+10+379):\n",
        "            if j < len1:\n",
        "                X_test3[cnt_i,j,:] = doc2vec_model[sen[i][0][j]]\n",
        "                avg_v1 += result_train1[i, tfidf1.vocabulary_[sen[i][0][j]]] * doc2vec_model[sen[i][0][j]]\n",
        "                avg_v2 += result_train2[i, tfidf2.vocabulary_[sen[i][0][j]]] * doc2vec_model[sen[i][0][j]]  \n",
        "                avg_v3 += result_train3[i, tfidf3.vocabulary_[sen[i][0][j]]] * doc2vec_model[sen[i][0][j]]\n",
        "            #elif j >= len1 and j < len1 + 379:\n",
        "               # X_test3[cnt_i,j,:] = glove_data[i, j-len1, :]\n",
        "            elif j == len1 + 379:\n",
        "                X_test3[cnt_i,j,:] = tfidf_data1[i]\n",
        "            elif j == len1 + 380:\n",
        "                X_test3[cnt_i,j,:] = tfidf_data2[i]\n",
        "            elif j == len1+381:\n",
        "                X_test3[cnt_i,j,:] = tfidf_data3[i]\n",
        "            elif j == len1+382:\n",
        "                X_test3[cnt_i,j,:] = avg_v1\n",
        "            elif j == len1+383:\n",
        "                X_test3[cnt_i,j,:] = avg_v2\n",
        "            elif j == len1+384:\n",
        "                X_test3[cnt_i,j,:] = avg_v3\n",
        "            elif j == len1+385:\n",
        "                X_test3[cnt_i,j,:] = final_pos_tags_data[i]\n",
        "            elif j == len1+386:\n",
        "                X_test3[cnt_i,j,:] = final_pos_data[i]\n",
        "            elif j == len1+387:\n",
        "                X_test3[cnt_i,j,:] = final_tokens_data[i]\n",
        "            elif j == len1+388:\n",
        "                X_test3[cnt_i,j,:] = final_dep_data[i]\n",
        "            else:\n",
        "                X_test3[cnt_i,j,:] = empty_word\n",
        "\n",
        "        Y_test3[cnt_i,:] = dummy_y[i]\n",
        "        cnt_i += 1\n",
        "        \n",
        "    return X_train3, X_test3, Y_train3, Y_test3\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4R2wRTOwKQ1",
        "outputId": "e462db9d-c73f-496a-b454-5f7ee140587f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, 759, 128)          327808    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 759, 128)          0         \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 379, 128)         0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 379, 128)          0         \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 100)              71600     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 4)                 404       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 399,812\n",
            "Trainable params: 399,812\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# Setting the layers of CNN-BiLSTM model and training the model\n",
        "model = Sequential()\n",
        "# Changing Activation Functions as required for the model\n",
        "#model.add(Conv1D(filters=128, kernel_size=5, padding='same', activation='relu', input_shape=(max(maxlength)+10+379,512)))\n",
        "#model.add(Conv1D(filters=128, kernel_size=5, padding='same', activation='selu', input_shape=(max(maxlength)+10+379,512)))\n",
        "#model.add(Conv1D(filters=128, kernel_size=5, padding='same', activation='tanh', input_shape=(max(maxlength)+10+379,512)))\n",
        "#model.add(Conv1D(filters=128, kernel_size=5, padding='same', activation='sigmoid', input_shape=(max(maxlength)+10+379,512)))\n",
        "model.add(Conv1D(filters=128, kernel_size=5, padding='same', activation='swish', input_shape=(max(maxlength)+10+379,512)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Dropout(0.25))\n",
        "# Removing layers as the required output is achieved and computational power is reduced\n",
        "#model.add(Conv1D(filters=32, kernel_size=7, padding='same', activation='swish'))\n",
        "#model.add(Dropout(0.25))\n",
        "#model.add(MaxPooling1D(pool_size=2))\n",
        "#model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Bidirectional(LSTM(50, dropout=0.25, recurrent_dropout=0.2)))\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXOnsJJ-wKQ1",
        "outputId": "42a68dc2-a3ba-4a21-cf75-c698f9a8c914"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.6564 - accuracy: 0.2681\n",
            "Epoch 1: val_loss improved from inf to 0.57060, saving model to weights.best.from_scratch0.hdf5\n",
            "3/3 [==============================] - 24s 5s/step - loss: 0.6564 - accuracy: 0.2681 - val_loss: 0.5706 - val_accuracy: 0.2500\n",
            "Epoch 2/10\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.5607 - accuracy: 0.2778\n",
            "Epoch 2: val_loss improved from 0.57060 to 0.54007, saving model to weights.best.from_scratch0.hdf5\n",
            "3/3 [==============================] - 10s 3s/step - loss: 0.5607 - accuracy: 0.2778 - val_loss: 0.5401 - val_accuracy: 0.4187\n",
            "Epoch 3/10\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.5376 - accuracy: 0.4292\n",
            "Epoch 3: val_loss improved from 0.54007 to 0.52463, saving model to weights.best.from_scratch0.hdf5\n",
            "3/3 [==============================] - 11s 4s/step - loss: 0.5376 - accuracy: 0.4292 - val_loss: 0.5246 - val_accuracy: 0.4875\n",
            "Epoch 4/10\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.5204 - accuracy: 0.5285\n",
            "Epoch 4: val_loss improved from 0.52463 to 0.50067, saving model to weights.best.from_scratch0.hdf5\n",
            "3/3 [==============================] - 10s 3s/step - loss: 0.5204 - accuracy: 0.5285 - val_loss: 0.5007 - val_accuracy: 0.6313\n",
            "Epoch 5/10\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4951 - accuracy: 0.6389\n",
            "Epoch 5: val_loss improved from 0.50067 to 0.45532, saving model to weights.best.from_scratch0.hdf5\n",
            "3/3 [==============================] - 10s 3s/step - loss: 0.4951 - accuracy: 0.6389 - val_loss: 0.4553 - val_accuracy: 0.7125\n",
            "Epoch 6/10\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4600 - accuracy: 0.7028\n",
            "Epoch 6: val_loss improved from 0.45532 to 0.41859, saving model to weights.best.from_scratch0.hdf5\n",
            "3/3 [==============================] - 10s 3s/step - loss: 0.4600 - accuracy: 0.7028 - val_loss: 0.4186 - val_accuracy: 0.6875\n",
            "Epoch 7/10\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4258 - accuracy: 0.7104\n",
            "Epoch 7: val_loss improved from 0.41859 to 0.38379, saving model to weights.best.from_scratch0.hdf5\n",
            "3/3 [==============================] - 10s 3s/step - loss: 0.4258 - accuracy: 0.7104 - val_loss: 0.3838 - val_accuracy: 0.7250\n",
            "Epoch 8/10\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3911 - accuracy: 0.7778\n",
            "Epoch 8: val_loss improved from 0.38379 to 0.35339, saving model to weights.best.from_scratch0.hdf5\n",
            "3/3 [==============================] - 11s 4s/step - loss: 0.3911 - accuracy: 0.7778 - val_loss: 0.3534 - val_accuracy: 0.8125\n",
            "Epoch 9/10\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3567 - accuracy: 0.8049\n",
            "Epoch 9: val_loss improved from 0.35339 to 0.33471, saving model to weights.best.from_scratch0.hdf5\n",
            "3/3 [==============================] - 10s 3s/step - loss: 0.3567 - accuracy: 0.8049 - val_loss: 0.3347 - val_accuracy: 0.8125\n",
            "Epoch 10/10\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3270 - accuracy: 0.8368\n",
            "Epoch 10: val_loss improved from 0.33471 to 0.31611, saving model to weights.best.from_scratch0.hdf5\n",
            "3/3 [==============================] - 10s 3s/step - loss: 0.3270 - accuracy: 0.8368 - val_loss: 0.3161 - val_accuracy: 0.7937\n",
            "0.775\n",
            "Epoch 1/10\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3042 - accuracy: 0.8313\n",
            "Epoch 1: val_loss improved from inf to 0.23221, saving model to weights.best.from_scratch1.hdf5\n",
            "3/3 [==============================] - 10s 3s/step - loss: 0.3042 - accuracy: 0.8313 - val_loss: 0.2322 - val_accuracy: 0.9062\n",
            "Epoch 2/10\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2825 - accuracy: 0.8396\n",
            "Epoch 2: val_loss improved from 0.23221 to 0.21623, saving model to weights.best.from_scratch1.hdf5\n",
            "3/3 [==============================] - 11s 4s/step - loss: 0.2825 - accuracy: 0.8396 - val_loss: 0.2162 - val_accuracy: 0.9000\n",
            "Epoch 3/10\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2587 - accuracy: 0.8521\n",
            "Epoch 3: val_loss improved from 0.21623 to 0.19452, saving model to weights.best.from_scratch1.hdf5\n",
            "3/3 [==============================] - 10s 3s/step - loss: 0.2587 - accuracy: 0.8521 - val_loss: 0.1945 - val_accuracy: 0.9062\n",
            "Epoch 4/10\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2360 - accuracy: 0.8694\n",
            "Epoch 4: val_loss improved from 0.19452 to 0.18712, saving model to weights.best.from_scratch1.hdf5\n",
            "3/3 [==============================] - 10s 4s/step - loss: 0.2360 - accuracy: 0.8694 - val_loss: 0.1871 - val_accuracy: 0.8813\n",
            "Epoch 5/10\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2198 - accuracy: 0.8764\n",
            "Epoch 5: val_loss improved from 0.18712 to 0.17891, saving model to weights.best.from_scratch1.hdf5\n",
            "3/3 [==============================] - 10s 3s/step - loss: 0.2198 - accuracy: 0.8764 - val_loss: 0.1789 - val_accuracy: 0.8687\n",
            "Epoch 6/10\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2079 - accuracy: 0.8785\n",
            "Epoch 6: val_loss improved from 0.17891 to 0.15791, saving model to weights.best.from_scratch1.hdf5\n",
            "3/3 [==============================] - 10s 3s/step - loss: 0.2079 - accuracy: 0.8785 - val_loss: 0.1579 - val_accuracy: 0.9250\n",
            "Epoch 7/10\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1954 - accuracy: 0.8882\n",
            "Epoch 7: val_loss did not improve from 0.15791\n",
            "3/3 [==============================] - 10s 3s/step - loss: 0.1954 - accuracy: 0.8882 - val_loss: 0.1842 - val_accuracy: 0.8625\n",
            "Epoch 8/10\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1809 - accuracy: 0.8979\n",
            "Epoch 8: val_loss improved from 0.15791 to 0.14690, saving model to weights.best.from_scratch1.hdf5\n",
            "3/3 [==============================] - 10s 3s/step - loss: 0.1809 - accuracy: 0.8979 - val_loss: 0.1469 - val_accuracy: 0.9125\n",
            "Epoch 9/10\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1721 - accuracy: 0.9083\n",
            "Epoch 9: val_loss did not improve from 0.14690\n",
            "3/3 [==============================] - 10s 4s/step - loss: 0.1721 - accuracy: 0.9083 - val_loss: 0.1798 - val_accuracy: 0.8562\n",
            "Epoch 10/10\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1571 - accuracy: 0.9215\n",
            "Epoch 10: val_loss did not improve from 0.14690\n",
            "3/3 [==============================] - 10s 3s/step - loss: 0.1571 - accuracy: 0.9215 - val_loss: 0.1557 - val_accuracy: 0.8875\n",
            "0.9125\n",
            "Epoch 1/10\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1719 - accuracy: 0.8986\n",
            "Epoch 1: val_loss improved from inf to 0.16731, saving model to weights.best.from_scratch2.hdf5\n",
            "3/3 [==============================] - 11s 4s/step - loss: 0.1719 - accuracy: 0.8986 - val_loss: 0.1673 - val_accuracy: 0.8938\n",
            "Epoch 2/10\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1583 - accuracy: 0.9111\n",
            "Epoch 2: val_loss improved from 0.16731 to 0.16126, saving model to weights.best.from_scratch2.hdf5\n",
            "3/3 [==============================] - 10s 3s/step - loss: 0.1583 - accuracy: 0.9111 - val_loss: 0.1613 - val_accuracy: 0.8938\n",
            "Epoch 3/10\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1425 - accuracy: 0.9340\n",
            "Epoch 3: val_loss did not improve from 0.16126\n",
            "3/3 [==============================] - 10s 3s/step - loss: 0.1425 - accuracy: 0.9340 - val_loss: 0.1692 - val_accuracy: 0.8687\n",
            "Epoch 4/10\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1369 - accuracy: 0.9278\n",
            "Epoch 4: val_loss improved from 0.16126 to 0.15946, saving model to weights.best.from_scratch2.hdf5\n",
            "3/3 [==============================] - 10s 4s/step - loss: 0.1369 - accuracy: 0.9278 - val_loss: 0.1595 - val_accuracy: 0.8938\n",
            "Epoch 5/10\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1300 - accuracy: 0.9368\n",
            "Epoch 5: val_loss did not improve from 0.15946\n",
            "3/3 [==============================] - 10s 3s/step - loss: 0.1300 - accuracy: 0.9368 - val_loss: 0.1712 - val_accuracy: 0.8813\n",
            "Epoch 6/10\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1189 - accuracy: 0.9438\n",
            "Epoch 6: val_loss did not improve from 0.15946\n",
            "3/3 [==============================] - 10s 3s/step - loss: 0.1189 - accuracy: 0.9438 - val_loss: 0.1723 - val_accuracy: 0.8625\n",
            "Epoch 7/10\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1117 - accuracy: 0.9507\n",
            "Epoch 7: val_loss improved from 0.15946 to 0.15751, saving model to weights.best.from_scratch2.hdf5\n",
            "3/3 [==============================] - 10s 3s/step - loss: 0.1117 - accuracy: 0.9507 - val_loss: 0.1575 - val_accuracy: 0.8938\n",
            "Epoch 8/10\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1050 - accuracy: 0.9563\n",
            "Epoch 8: val_loss improved from 0.15751 to 0.15592, saving model to weights.best.from_scratch2.hdf5\n",
            "3/3 [==============================] - 10s 3s/step - loss: 0.1050 - accuracy: 0.9563 - val_loss: 0.1559 - val_accuracy: 0.8813\n",
            "Epoch 9/10\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0959 - accuracy: 0.9611\n",
            "Epoch 9: val_loss did not improve from 0.15592\n",
            "3/3 [==============================] - 10s 3s/step - loss: 0.0959 - accuracy: 0.9611 - val_loss: 0.1688 - val_accuracy: 0.8625\n",
            "Epoch 10/10\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0867 - accuracy: 0.9688\n",
            "Epoch 10: val_loss did not improve from 0.15592\n",
            "3/3 [==============================] - 10s 3s/step - loss: 0.0867 - accuracy: 0.9688 - val_loss: 0.1560 - val_accuracy: 0.8813\n",
            "0.875\n",
            "Epoch 1/10\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1033 - accuracy: 0.9542\n",
            "Epoch 1: val_loss improved from inf to 0.07748, saving model to weights.best.from_scratch3.hdf5\n",
            "3/3 [==============================] - 10s 3s/step - loss: 0.1033 - accuracy: 0.9542 - val_loss: 0.0775 - val_accuracy: 0.9688\n",
            "Epoch 2/10\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0974 - accuracy: 0.9604\n",
            "Epoch 2: val_loss did not improve from 0.07748\n",
            "3/3 [==============================] - 10s 3s/step - loss: 0.0974 - accuracy: 0.9604 - val_loss: 0.0775 - val_accuracy: 0.9688\n",
            "Epoch 3/10\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0900 - accuracy: 0.9618\n",
            "Epoch 3: val_loss improved from 0.07748 to 0.07591, saving model to weights.best.from_scratch3.hdf5\n",
            "3/3 [==============================] - 10s 3s/step - loss: 0.0900 - accuracy: 0.9618 - val_loss: 0.0759 - val_accuracy: 0.9500\n",
            "Epoch 4/10\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0827 - accuracy: 0.9708\n",
            "Epoch 4: val_loss improved from 0.07591 to 0.07225, saving model to weights.best.from_scratch3.hdf5\n",
            "3/3 [==============================] - 10s 3s/step - loss: 0.0827 - accuracy: 0.9708 - val_loss: 0.0723 - val_accuracy: 0.9563\n",
            "Epoch 5/10\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0777 - accuracy: 0.9736\n",
            "Epoch 5: val_loss did not improve from 0.07225\n",
            "3/3 [==============================] - 10s 3s/step - loss: 0.0777 - accuracy: 0.9736 - val_loss: 0.0728 - val_accuracy: 0.9500\n",
            "Epoch 6/10\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0691 - accuracy: 0.9792\n",
            "Epoch 6: val_loss did not improve from 0.07225\n",
            "3/3 [==============================] - 10s 3s/step - loss: 0.0691 - accuracy: 0.9792 - val_loss: 0.0728 - val_accuracy: 0.9438\n",
            "Epoch 7/10\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9750\n",
            "Epoch 7: val_loss improved from 0.07225 to 0.06841, saving model to weights.best.from_scratch3.hdf5\n",
            "3/3 [==============================] - 10s 3s/step - loss: 0.0699 - accuracy: 0.9750 - val_loss: 0.0684 - val_accuracy: 0.9625\n",
            "Epoch 8/10\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0597 - accuracy: 0.9826\n",
            "Epoch 8: val_loss improved from 0.06841 to 0.06404, saving model to weights.best.from_scratch3.hdf5\n",
            "3/3 [==============================] - 10s 3s/step - loss: 0.0597 - accuracy: 0.9826 - val_loss: 0.0640 - val_accuracy: 0.9625\n",
            "Epoch 9/10\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0563 - accuracy: 0.9840\n",
            "Epoch 9: val_loss did not improve from 0.06404\n",
            "3/3 [==============================] - 10s 3s/step - loss: 0.0563 - accuracy: 0.9840 - val_loss: 0.0659 - val_accuracy: 0.9625\n",
            "Epoch 10/10\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0527 - accuracy: 0.9861\n",
            "Epoch 10: val_loss did not improve from 0.06404\n",
            "3/3 [==============================] - 10s 3s/step - loss: 0.0527 - accuracy: 0.9861 - val_loss: 0.0749 - val_accuracy: 0.9375\n"
          ]
        }
      ],
      "source": [
        "# Running the model and calculating the accuracy, loss, validation accuracy and validation loss at each iteration, each epoch\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.callbacks import ModelCheckpoint \n",
        "\n",
        "final_accuracies = []\n",
        "\n",
        "for i in range(4):\n",
        "    filename = 'weights.best.from_scratch%s.hdf5' % i\n",
        "    checkpointer = ModelCheckpoint(filepath=filename, verbose=1, save_best_only=True)\n",
        "    X_train3, X_test3, Y_train3, Y_test3 = extractTrainingAndTestingData(i)\n",
        "    model.fit(X_train3, Y_train3, epochs=10, batch_size=512, callbacks=[checkpointer], validation_data=(X_test3, Y_test3))\n",
        "    model.load_weights(filename)\n",
        "    predicted = np.rint(model.predict(X_test3))\n",
        "    final_accuracies.append(accuracy_score(Y_test3, predicted))\n",
        "    print(accuracy_score(Y_test3, predicted))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTB-7-T_wKQ1",
        "outputId": "9d994fca-d2f3-4986-a40d-bce6607aff41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "91.38%\n"
          ]
        }
      ],
      "source": [
        "# Calculating the overall accuracy\n",
        "print(sum(final_accuracies) / len(final_accuracies)*100,\"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETOVViZSwKQ2"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "name": "CNN-BiLSTM_using_Doc2Vec_and_Swish.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
